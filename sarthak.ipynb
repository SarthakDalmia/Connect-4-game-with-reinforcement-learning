{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "g_LxEq1M2WJ5"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import enum\n",
        "import random\n",
        "import numpy as np\n",
        "import pickle\n",
        "import ast\n",
        "import gzip\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LGITikDg2WJ8"
      },
      "outputs": [],
      "source": [
        "def PrintGrid(positions):\n",
        "    print('\\n'.join(' '.join(str(x) for x in row) for row in positions))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Rmbj_y412WJ9"
      },
      "outputs": [],
      "source": [
        "class Outcome(enum.Enum):\n",
        "    NonLeaf = 2\n",
        "    Win =1\n",
        "    Draw =0\n",
        "    Loss =-1\n",
        "        \n",
        "def checkDiagonal(grid, startRow, startCol, rowUpdate, colUpdate):\n",
        "    consecutiveNegOnes = 0\n",
        "    consecutiveOnes = 0\n",
        "\n",
        "    row = startRow\n",
        "    col = startCol\n",
        "    while  col >=0 and row >= 0 and col < grid.shape[1] and row < grid.shape[0]:\n",
        "        if grid[row][col]==1:\n",
        "            consecutiveNegOnes=0\n",
        "            consecutiveOnes+=1\n",
        "        \n",
        "        elif grid[row][col]==-1:\n",
        "            consecutiveNegOnes+=1\n",
        "            consecutiveOnes=0\n",
        "        else:\n",
        "            consecutiveNegOnes=0\n",
        "            consecutiveOnes=0\n",
        "        if consecutiveNegOnes >= 4:\n",
        "            return Outcome.Loss\n",
        "        if consecutiveOnes >= 4:\n",
        "            return Outcome.Win\n",
        "\n",
        "        col+=colUpdate\n",
        "        row+=rowUpdate\n",
        "    \n",
        "    return Outcome.Draw\n",
        "\n",
        "def winOrLoss(grid):\n",
        "    # Calculate state if not win or loss\n",
        "    outcome = Outcome.Draw\n",
        "    \n",
        "    for row in range(grid.shape[0]):\n",
        "        tempOutcome = checkDiagonal(grid,row, 0, 0, 1)\n",
        "        if tempOutcome != Outcome.Draw:\n",
        "            return tempOutcome\n",
        "    \n",
        "    for col in range(grid.shape[1]):\n",
        "        tempOutcome = checkDiagonal(grid, 0, col, 1, 0)\n",
        "        if tempOutcome != Outcome.Draw:\n",
        "            return tempOutcome\n",
        "    \n",
        "    for diags in range(10):\n",
        "        row = max(0, 5-diags)\n",
        "        col = max(0,diags-5)\n",
        "        tempOutcome = checkDiagonal(grid, row, col, 1, 1)\n",
        "        if tempOutcome != Outcome.Draw:\n",
        "            return tempOutcome\n",
        "\n",
        "    for diags in range(10):\n",
        "        row = min(5, diags)\n",
        "        col = max(0,diags-5)\n",
        "        tempOutcome = checkDiagonal(grid, row, col, -1, 1)\n",
        "        if tempOutcome != Outcome.Draw:\n",
        "            return tempOutcome\n",
        "\n",
        "    for col in range(grid.shape[1]):\n",
        "        if grid[0][col] == 0:\n",
        "            return Outcome.NonLeaf\n",
        "\n",
        "    return outcome\n",
        "\n",
        "def getPossibleMoves(grid):\n",
        "    possibleMoves = []\n",
        "    for col in range(grid.shape[1]):\n",
        "        if grid[0,col] == 0:\n",
        "            possibleMoves.append(col)\n",
        "    return np.array(possibleMoves).ravel()\n",
        "\n",
        "def createChild(grid, move):\n",
        "    last_zero_index = None\n",
        "    for row in range(grid.shape[0]):\n",
        "        if grid[row][move] != 0:\n",
        "            break\n",
        "        last_zero_index = row\n",
        "    \n",
        "    if last_zero_index != None:\n",
        "        child_grid = grid.copy()\n",
        "        child_grid[last_zero_index][move] = 1\n",
        "        child_grid = -1*child_grid\n",
        "        return child_grid\n",
        "\n",
        "def to_immutable(grid):\n",
        "    return tuple(map(tuple, grid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "nkkvHTlN2WKA"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Node:\n",
        "    \n",
        "    def __init__(self, board, parent):\n",
        "        self.board = board\n",
        "        self.parent = parent\n",
        "        self.children = {}\n",
        "        self.rewards = 0\n",
        "        self.plays = 0\n",
        "        \n",
        "    def registerOutcome(self, outcome):\n",
        "        self.plays += 1\n",
        "        if outcome == Outcome.Loss:\n",
        "            self.rewards += -1\n",
        "        elif outcome == Outcome.Win:\n",
        "            self.rewards += 1\n",
        "    \n",
        "    def hasChildren(self):\n",
        "        return bool(self.children)\n",
        "\n",
        "    def computeChildren(self):\n",
        "        self.children = {}\n",
        "        for move in range(self.board.shape[1]):\n",
        "            child = createChild(self.board, move)\n",
        "            if(child is not None):\n",
        "                self.children[move] = Node(child, self)\n",
        "\n",
        "    def getOutcome(self):\n",
        "        return winOrLoss(self.board)\n",
        "    \n",
        "    def uctValue(self, uctConstant):\n",
        "        if  self.plays != 0 and self.parent != None:\n",
        "            temp = self.rewards/self.plays\n",
        "            temp += (uctConstant * math.sqrt(math.log(self.parent.plays)/self.plays))\n",
        "            return temp\n",
        "        elif  self.plays == 0 and self.parent != None:\n",
        "            return math.inf\n",
        "    \n",
        "    def selectChild(self, uctConstant):\n",
        "        bestUctValue = -math.inf\n",
        "        for child in self.children.values():\n",
        "            if bestUctValue < child.uctValue(uctConstant):\n",
        "                bestUctValue = child.uctValue(uctConstant)\n",
        "        childList = []\n",
        "        for child in self.children.values():\n",
        "            if(child.uctValue(uctConstant)==bestUctValue):\n",
        "                childList.append(child)\n",
        "        return random.choice(childList)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "arHSu-hk2WKB"
      },
      "outputs": [],
      "source": [
        "\n",
        "class MonteCarloTreeSearchAgent:\n",
        "    def __init__(self, numSims, board = np.zeros((6,5)).astype(int), uctConst = 1.4):\n",
        "        self.numSims = numSims\n",
        "        self.root = Node(board, None)\n",
        "        self.uctConst = uctConst\n",
        "        self.firstMove = True\n",
        "\n",
        "    def reset(self, board = np.zeros((6,5)).astype(int)):\n",
        "        self.root = Node(board, None)\n",
        "        \n",
        "        self.firstMove = True\n",
        "\n",
        "    def registerMove(self, move):\n",
        "        if not self.root.hasChildren():\n",
        "            self.root.computeChildren()\n",
        "        self.root = self.root.children[move]\n",
        "        self.root.parent = None\n",
        "\n",
        "    def getMove(self):\n",
        "        \n",
        "        for sim in range(self.numSims):\n",
        "            currNode, depth = self.selection()\n",
        "            \n",
        "            if depth<4 or not self.firstMove:\n",
        "              currNode = self.expansion(currNode=currNode)\n",
        "            \n",
        "            outcome = self.simulation(currBoard=currNode.board)\n",
        "            \n",
        "            self.backprop(currNode=currNode, outcome=outcome)\n",
        "\n",
        "        # Calculate best move\n",
        "        bestMove = None\n",
        "        maxPlays = -1\n",
        "        for move in self.root.children:\n",
        "            if self.root.children[move].plays > maxPlays:\n",
        "                maxPlays = self.root.children[move].plays\n",
        "                bestMove = move\n",
        "\n",
        "        bestValue = self.root.children[bestMove].uctValue(self.uctConst)\n",
        "\n",
        "        self.registerMove(bestMove)\n",
        "        self.firstMove = False\n",
        "        return bestMove, bestValue\n",
        "\n",
        "    def selection(self):\n",
        "        depth = 0\n",
        "        currNode = self.root\n",
        "        while currNode.hasChildren() and currNode.getOutcome()==Outcome.NonLeaf:\n",
        "            currNode = currNode.selectChild(self.uctConst)\n",
        "            depth+=1\n",
        "        return currNode, depth\n",
        "    \n",
        "    def expansion(self, currNode):\n",
        "        if currNode.getOutcome() == Outcome.NonLeaf:\n",
        "            currNode.computeChildren()\n",
        "            return currNode.selectChild(self.uctConst)\n",
        "        else:\n",
        "            return currNode\n",
        "    \n",
        "    def simulation(self, currBoard):\n",
        "        if winOrLoss(currBoard) == Outcome.NonLeaf:\n",
        "            actions = getPossibleMoves(currBoard)\n",
        "            nextBoard = createChild(currBoard, random.choice(actions))\n",
        "\n",
        "            outcome = self.simulation(nextBoard)\n",
        "            if outcome == Outcome.Loss:\n",
        "                return Outcome.Win\n",
        "            elif outcome == Outcome.Win:\n",
        "                return Outcome.Loss\n",
        "            else:\n",
        "                return outcome\n",
        "            \n",
        "        else:\n",
        "            outcome = winOrLoss(currBoard)\n",
        "            if outcome == Outcome.Loss:\n",
        "                return Outcome.Win\n",
        "            elif outcome == Outcome.Win:\n",
        "                return Outcome.Loss\n",
        "            else:\n",
        "                return outcome\n",
        "\n",
        "    def backprop(self, currNode, outcome):\n",
        "        while currNode!=None:\n",
        "            currNode.registerOutcome(outcome)\n",
        "            if outcome == Outcome.Loss:\n",
        "                outcome= Outcome.Win\n",
        "            elif outcome == Outcome.Win:\n",
        "                outcome= Outcome.Loss\n",
        "            else:\n",
        "                outcome= outcome\n",
        "            currNode = currNode.parent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "T3WTu-ju2WKC"
      },
      "outputs": [],
      "source": [
        "'''make it converge faster'''\n",
        "class AfterstatesQLearningAgent:\n",
        "    \n",
        "    def __init__(self, board = np.zeros((4,5)).astype(int), epsilon=0.1, alpha=1, gamma=0.5):\n",
        "        self.currBoard = board\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "        self.oldBoard = None\n",
        "        self.valueMap = {}\n",
        "\n",
        "    def loadValueMap(self, filename):\n",
        "        sfile = gzip.GzipFile(filename, 'r')\n",
        "        self.valueMap = pickle.load(sfile)\n",
        "\n",
        "    def saveValueMap(self, filename):\n",
        "        sfile = gzip.GzipFile(filename, 'w')\n",
        "        pickle.dump(self.valueMap, sfile)\n",
        "\n",
        "    def reset(self, board = np.zeros((4,5)).astype(int)):\n",
        "        self.currBoard = board\n",
        "        self.oldBoard = None\n",
        "    \n",
        "    def registerMove(self, move):\n",
        "        self.oldBoard = self.currBoard\n",
        "        self.currBoard = createChild(self.currBoard, move)\n",
        "        if winOrLoss(self.currBoard)!=Outcome.NonLeaf:\n",
        "            if not self.oldBoard is None:\n",
        "                if winOrLoss(self.currBoard) == Outcome.Loss:\n",
        "                    self.valueMap[to_immutable(self.oldBoard)] = -1\n",
        "                elif winOrLoss(self.currBoard) == Outcome.Win:\n",
        "                    self.valueMap[to_immutable(self.oldBoard)] = 1\n",
        "                elif winOrLoss(self.currBoard) == Outcome.Draw:\n",
        "                    self.valueMap[to_immutable(self.oldBoard)] = -1\n",
        "\n",
        "    def getMove(self):\n",
        "\n",
        "        nextBoard, move, maxValuePossible = self.selectBoard()\n",
        "\n",
        "        if not self.oldBoard is None:\n",
        "            outcome = winOrLoss(nextBoard)\n",
        "            if outcome == Outcome.Win:\n",
        "                outcome= Outcome.Loss\n",
        "            elif outcome == Outcome.Loss:\n",
        "                outcome= Outcome.Win\n",
        "            else:\n",
        "                outcome= outcome\n",
        "            if outcome == Outcome.Loss:\n",
        "                reward = -1\n",
        "            elif outcome == Outcome.Win:\n",
        "                reward = 1\n",
        "            elif outcome == Outcome.Draw:\n",
        "                reward = -1\n",
        "            else:\n",
        "                reward = 0\n",
        "            self.valueMap[to_immutable(self.oldBoard)] = (1-self.alpha)*self.valueMap.get(to_immutable(self.oldBoard),0) + self.alpha * (reward + self.gamma*maxValuePossible)\n",
        "\n",
        "        self.currBoard = nextBoard\n",
        "\n",
        "        return move, self.valueMap.get(to_immutable(nextBoard),0)\n",
        "    \n",
        "    def selectBoard(self):\n",
        "        possibleMoves = getPossibleMoves(self.currBoard)\n",
        "        possibleBoards = [createChild(self.currBoard, move) for move in possibleMoves]\n",
        "\n",
        "        maxQ = -math.inf\n",
        "        for availableBoard in possibleBoards:\n",
        "            if maxQ < self.valueMap.get(to_immutable(availableBoard),0):\n",
        "                maxQ = self.valueMap.get(to_immutable(availableBoard),0)\n",
        "        \n",
        "        greedyIndex = -1\n",
        "        for index in range(len(possibleBoards)):\n",
        "            if self.valueMap.get(to_immutable(possibleBoards[index]),0)==maxQ:\n",
        "                greedyIndex = index\n",
        "                break\n",
        "        \n",
        "        if random.random() >= 1-self.epsilon:\n",
        "            index = random.randrange(len(possibleMoves))\n",
        "            return possibleBoards[index], possibleMoves[index], maxQ\n",
        "        else:\n",
        "            return possibleBoards[greedyIndex], possibleMoves[greedyIndex], maxQ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "i-K82ZED2WKB"
      },
      "outputs": [],
      "source": [
        "def getAgentName(agent):\n",
        "    if isinstance(agent, AfterstatesQLearningAgent):\n",
        "        return \"Q-Learning\"\n",
        "    else:\n",
        "        return \"MCTS\"\n",
        "\n",
        "def play(board, firstAgent, secondAgent):\n",
        "\n",
        "    print(\"**** New Game *****\")\n",
        "\n",
        "    firstAgent.reset(board)\n",
        "    secondAgent.reset(board)\n",
        "\n",
        "    numMoves = 0\n",
        "\n",
        "    currAgentIndex = 0\n",
        "    \n",
        "    agents = [firstAgent, secondAgent]\n",
        "\n",
        "    while winOrLoss(board) == Outcome.NonLeaf:\n",
        "        numMoves+=1\n",
        "\n",
        "        move, value = agents[currAgentIndex].getMove()\n",
        "        agents[1-currAgentIndex].registerMove(move)\n",
        "\n",
        "        print(\"Player\", currAgentIndex+1, \"(\"+getAgentName(agents[currAgentIndex])+\")\")\n",
        "        print(\"Action Selected :\", move)\n",
        "        print(\"Value of next state according to\", getAgentName(agents[currAgentIndex]), \":\", \"{:.4f}\".format(round(value,ndigits=4)))\n",
        "        \n",
        "        board = createChild(board, move)\n",
        "        \n",
        "        if currAgentIndex != 0:\n",
        "            printArray = board\n",
        "        else:\n",
        "            printArray = -1 * board\n",
        "\n",
        "        printArray = np.where(printArray == -1, 2, printArray)\n",
        "        PrintGrid(printArray)\n",
        "        \n",
        "        currAgentIndex = 1-currAgentIndex\n",
        "\n",
        "    print(\"Player\", str(currAgentIndex+1), \"has\", str(winOrLoss(board))+\".\", \"Total moves =\", str(numMoves)+\".\")\n",
        "        \n",
        "    if winOrLoss(board) == Outcome.Win:\n",
        "        return currAgentIndex\n",
        "    elif winOrLoss(board) == Outcome.Loss:\n",
        "        return 1-currAgentIndex\n",
        "    else:\n",
        "        return 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**** New Game *****\n",
            "Player 1 (Q-Learning)\n",
            "Action Selected : 0\n",
            "Value of next state according to Q-Learning : 0.0000\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "1 0 0 0 0\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 4\n",
            "Value of next state according to MCTS : 0.8115\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "1 0 0 0 2\n",
            "\n",
            "Player 1 (Q-Learning)\n",
            "Action Selected : 0\n",
            "Value of next state according to Q-Learning : 0.0000\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "1 0 0 0 0\n",
            "1 0 0 0 2\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 3\n",
            "Value of next state according to MCTS : 0.8514\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "1 0 0 0 0\n",
            "1 0 0 2 2\n",
            "\n",
            "Player 1 (Q-Learning)\n",
            "Action Selected : 0\n",
            "Value of next state according to Q-Learning : 0.0000\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "1 0 0 0 0\n",
            "1 0 0 0 0\n",
            "1 0 0 2 2\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 2\n",
            "Value of next state according to MCTS : 0.5553\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "1 0 0 0 0\n",
            "1 0 0 0 0\n",
            "1 0 2 2 2\n",
            "\n",
            "Player 1 (Q-Learning)\n",
            "Action Selected : 0\n",
            "Value of next state according to Q-Learning : 0.0000\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "1 0 0 0 0\n",
            "1 0 0 0 0\n",
            "1 0 0 0 0\n",
            "1 0 2 2 2\n",
            "\n",
            "Player 2 has Outcome.Loss. Total moves = 7.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "play(np.zeros((6,5)).astype(int), AfterstatesQLearningAgent(), MonteCarloTreeSearchAgent(50))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ut3TGoFn2WKI"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "GR0Td4kf2WKJ",
        "outputId": "c8e864ca-4355-439a-c65f-39f6be5cc290"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent 0 is 40\n",
            "**** New Game *****\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 3\n",
            "Value of next state according to MCTS : 0.9016\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 1 0\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 4\n",
            "Value of next state according to MCTS : 0.4790\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 1 2\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 1\n",
            "Value of next state according to MCTS : 1.2925\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 1 0 1 2\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 2\n",
            "Value of next state according to MCTS : 0.3035\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 1 2 1 2\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 2\n",
            "Value of next state according to MCTS : 1.0552\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 1 0 0\n",
            "0 1 2 1 2\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 4\n",
            "Value of next state according to MCTS : 0.2126\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 1 0 2\n",
            "0 1 2 1 2\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 2\n",
            "Value of next state according to MCTS : 1.1204\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 1 0 0\n",
            "0 0 1 0 2\n",
            "0 1 2 1 2\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 4\n",
            "Value of next state according to MCTS : 0.0499\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 1 0 2\n",
            "0 0 1 0 2\n",
            "0 1 2 1 2\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 4\n",
            "Value of next state according to MCTS : 1.2633\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 1\n",
            "0 0 1 0 2\n",
            "0 0 1 0 2\n",
            "0 1 2 1 2\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 1\n",
            "Value of next state according to MCTS : -0.3089\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 1\n",
            "0 0 1 0 2\n",
            "0 2 1 0 2\n",
            "0 1 2 1 2\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 4\n",
            "Value of next state according to MCTS : 1.6628\n",
            "0 0 0 0 0\n",
            "0 0 0 0 1\n",
            "0 0 0 0 1\n",
            "0 0 1 0 2\n",
            "0 2 1 0 2\n",
            "0 1 2 1 2\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 2\n",
            "Value of next state according to MCTS : -0.0899\n",
            "0 0 0 0 0\n",
            "0 0 0 0 1\n",
            "0 0 2 0 1\n",
            "0 0 1 0 2\n",
            "0 2 1 0 2\n",
            "0 1 2 1 2\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 4\n",
            "Value of next state according to MCTS : 1.4892\n",
            "0 0 0 0 1\n",
            "0 0 0 0 1\n",
            "0 0 2 0 1\n",
            "0 0 1 0 2\n",
            "0 2 1 0 2\n",
            "0 1 2 1 2\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 1\n",
            "Value of next state according to MCTS : 0.1084\n",
            "0 0 0 0 1\n",
            "0 0 0 0 1\n",
            "0 0 2 0 1\n",
            "0 2 1 0 2\n",
            "0 2 1 0 2\n",
            "0 1 2 1 2\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 1\n",
            "Value of next state according to MCTS : 1.1331\n",
            "0 0 0 0 1\n",
            "0 0 0 0 1\n",
            "0 1 2 0 1\n",
            "0 2 1 0 2\n",
            "0 2 1 0 2\n",
            "0 1 2 1 2\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 1\n",
            "Value of next state according to MCTS : 0.1074\n",
            "0 0 0 0 1\n",
            "0 2 0 0 1\n",
            "0 1 2 0 1\n",
            "0 2 1 0 2\n",
            "0 2 1 0 2\n",
            "0 1 2 1 2\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 2\n",
            "Value of next state according to MCTS : 0.9913\n",
            "0 0 0 0 1\n",
            "0 2 1 0 1\n",
            "0 1 2 0 1\n",
            "0 2 1 0 2\n",
            "0 2 1 0 2\n",
            "0 1 2 1 2\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 0\n",
            "Value of next state according to MCTS : 0.1357\n",
            "0 0 0 0 1\n",
            "0 2 1 0 1\n",
            "0 1 2 0 1\n",
            "0 2 1 0 2\n",
            "0 2 1 0 2\n",
            "2 1 2 1 2\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 0\n",
            "Value of next state according to MCTS : 0.8329\n",
            "0 0 0 0 1\n",
            "0 2 1 0 1\n",
            "0 1 2 0 1\n",
            "0 2 1 0 2\n",
            "1 2 1 0 2\n",
            "2 1 2 1 2\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 0\n",
            "Value of next state according to MCTS : 0.0312\n",
            "0 0 0 0 1\n",
            "0 2 1 0 1\n",
            "0 1 2 0 1\n",
            "2 2 1 0 2\n",
            "1 2 1 0 2\n",
            "2 1 2 1 2\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 1\n",
            "Value of next state according to MCTS : 1.4309\n",
            "0 1 0 0 1\n",
            "0 2 1 0 1\n",
            "0 1 2 0 1\n",
            "2 2 1 0 2\n",
            "1 2 1 0 2\n",
            "2 1 2 1 2\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 2\n",
            "Value of next state according to MCTS : -0.4002\n",
            "0 1 2 0 1\n",
            "0 2 1 0 1\n",
            "0 1 2 0 1\n",
            "2 2 1 0 2\n",
            "1 2 1 0 2\n",
            "2 1 2 1 2\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 0\n",
            "Value of next state according to MCTS : 1.2310\n",
            "0 1 2 0 1\n",
            "0 2 1 0 1\n",
            "1 1 2 0 1\n",
            "2 2 1 0 2\n",
            "1 2 1 0 2\n",
            "2 1 2 1 2\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 0\n",
            "Value of next state according to MCTS : -0.6762\n",
            "0 1 2 0 1\n",
            "2 2 1 0 1\n",
            "1 1 2 0 1\n",
            "2 2 1 0 2\n",
            "1 2 1 0 2\n",
            "2 1 2 1 2\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 0\n",
            "Value of next state according to MCTS : 1.3804\n",
            "1 1 2 0 1\n",
            "2 2 1 0 1\n",
            "1 1 2 0 1\n",
            "2 2 1 0 2\n",
            "1 2 1 0 2\n",
            "2 1 2 1 2\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 3\n",
            "Value of next state according to MCTS : -0.8158\n",
            "1 1 2 0 1\n",
            "2 2 1 0 1\n",
            "1 1 2 0 1\n",
            "2 2 1 0 2\n",
            "1 2 1 2 2\n",
            "2 1 2 1 2\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 3\n",
            "Value of next state according to MCTS : 1.3094\n",
            "1 1 2 0 1\n",
            "2 2 1 0 1\n",
            "1 1 2 0 1\n",
            "2 2 1 1 2\n",
            "1 2 1 2 2\n",
            "2 1 2 1 2\n",
            "\n",
            "Player 2 has Outcome.Loss. Total moves = 27.\n",
            "**** New Game *****\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 1\n",
            "Value of next state according to MCTS : 0.9327\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 1 0 0 0\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 2\n",
            "Value of next state according to MCTS : 0.3470\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 1 2 0 0\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 1\n",
            "Value of next state according to MCTS : 1.2948\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 1 0 0 0\n",
            "0 1 2 0 0\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 1\n",
            "Value of next state according to MCTS : 0.2101\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 2 0 0 0\n",
            "0 1 0 0 0\n",
            "0 1 2 0 0\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 0\n",
            "Value of next state according to MCTS : 1.0005\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 2 0 0 0\n",
            "0 1 0 0 0\n",
            "1 1 2 0 0\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 2\n",
            "Value of next state according to MCTS : 0.5302\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 2 0 0 0\n",
            "0 1 2 0 0\n",
            "1 1 2 0 0\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 2\n",
            "Value of next state according to MCTS : 0.7126\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 2 1 0 0\n",
            "0 1 2 0 0\n",
            "1 1 2 0 0\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 3\n",
            "Value of next state according to MCTS : 0.3099\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 2 1 0 0\n",
            "0 1 2 0 0\n",
            "1 1 2 2 0\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 4\n",
            "Value of next state according to MCTS : 0.7813\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 2 1 0 0\n",
            "0 1 2 0 0\n",
            "1 1 2 2 1\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 1\n",
            "Value of next state according to MCTS : 0.4547\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 2 0 0 0\n",
            "0 2 1 0 0\n",
            "0 1 2 0 0\n",
            "1 1 2 2 1\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 2\n",
            "Value of next state according to MCTS : 0.8729\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 2 1 0 0\n",
            "0 2 1 0 0\n",
            "0 1 2 0 0\n",
            "1 1 2 2 1\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 1\n",
            "Value of next state according to MCTS : 0.3803\n",
            "0 0 0 0 0\n",
            "0 2 0 0 0\n",
            "0 2 1 0 0\n",
            "0 2 1 0 0\n",
            "0 1 2 0 0\n",
            "1 1 2 2 1\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 1\n",
            "Value of next state according to MCTS : 0.8534\n",
            "0 1 0 0 0\n",
            "0 2 0 0 0\n",
            "0 2 1 0 0\n",
            "0 2 1 0 0\n",
            "0 1 2 0 0\n",
            "1 1 2 2 1\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 2\n",
            "Value of next state according to MCTS : 0.3341\n",
            "0 1 0 0 0\n",
            "0 2 2 0 0\n",
            "0 2 1 0 0\n",
            "0 2 1 0 0\n",
            "0 1 2 0 0\n",
            "1 1 2 2 1\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 2\n",
            "Value of next state according to MCTS : 0.7392\n",
            "0 1 1 0 0\n",
            "0 2 2 0 0\n",
            "0 2 1 0 0\n",
            "0 2 1 0 0\n",
            "0 1 2 0 0\n",
            "1 1 2 2 1\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 0\n",
            "Value of next state according to MCTS : 0.6350\n",
            "0 1 1 0 0\n",
            "0 2 2 0 0\n",
            "0 2 1 0 0\n",
            "0 2 1 0 0\n",
            "2 1 2 0 0\n",
            "1 1 2 2 1\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 3\n",
            "Value of next state according to MCTS : 0.1735\n",
            "0 1 1 0 0\n",
            "0 2 2 0 0\n",
            "0 2 1 0 0\n",
            "0 2 1 0 0\n",
            "2 1 2 1 0\n",
            "1 1 2 2 1\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 4\n",
            "Value of next state according to MCTS : 0.7782\n",
            "0 1 1 0 0\n",
            "0 2 2 0 0\n",
            "0 2 1 0 0\n",
            "0 2 1 0 0\n",
            "2 1 2 1 2\n",
            "1 1 2 2 1\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 4\n",
            "Value of next state according to MCTS : 0.1546\n",
            "0 1 1 0 0\n",
            "0 2 2 0 0\n",
            "0 2 1 0 0\n",
            "0 2 1 0 1\n",
            "2 1 2 1 2\n",
            "1 1 2 2 1\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 4\n",
            "Value of next state according to MCTS : 0.9654\n",
            "0 1 1 0 0\n",
            "0 2 2 0 0\n",
            "0 2 1 0 2\n",
            "0 2 1 0 1\n",
            "2 1 2 1 2\n",
            "1 1 2 2 1\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 3\n",
            "Value of next state according to MCTS : 0.2338\n",
            "0 1 1 0 0\n",
            "0 2 2 0 0\n",
            "0 2 1 0 2\n",
            "0 2 1 1 1\n",
            "2 1 2 1 2\n",
            "1 1 2 2 1\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 3\n",
            "Value of next state according to MCTS : 1.0643\n",
            "0 1 1 0 0\n",
            "0 2 2 0 0\n",
            "0 2 1 2 2\n",
            "0 2 1 1 1\n",
            "2 1 2 1 2\n",
            "1 1 2 2 1\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 3\n",
            "Value of next state according to MCTS : -0.1570\n",
            "0 1 1 0 0\n",
            "0 2 2 1 0\n",
            "0 2 1 2 2\n",
            "0 2 1 1 1\n",
            "2 1 2 1 2\n",
            "1 1 2 2 1\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 3\n",
            "Value of next state according to MCTS : 1.2151\n",
            "0 1 1 2 0\n",
            "0 2 2 1 0\n",
            "0 2 1 2 2\n",
            "0 2 1 1 1\n",
            "2 1 2 1 2\n",
            "1 1 2 2 1\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 0\n",
            "Value of next state according to MCTS : -0.3427\n",
            "0 1 1 2 0\n",
            "0 2 2 1 0\n",
            "0 2 1 2 2\n",
            "1 2 1 1 1\n",
            "2 1 2 1 2\n",
            "1 1 2 2 1\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 0\n",
            "Value of next state according to MCTS : 1.1965\n",
            "0 1 1 2 0\n",
            "0 2 2 1 0\n",
            "2 2 1 2 2\n",
            "1 2 1 1 1\n",
            "2 1 2 1 2\n",
            "1 1 2 2 1\n",
            "\n",
            "Player 1 has Outcome.Loss. Total moves = 26.\n",
            "**** New Game *****\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 1\n",
            "Value of next state according to MCTS : 1.0906\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 1 0 0 0\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 3\n",
            "Value of next state according to MCTS : 0.3321\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 1 0 2 0\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 1\n",
            "Value of next state according to MCTS : 1.0517\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 1 0 0 0\n",
            "0 1 0 2 0\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 3\n",
            "Value of next state according to MCTS : 0.2165\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 1 0 2 0\n",
            "0 1 0 2 0\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 1\n",
            "Value of next state according to MCTS : 1.1607\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 1 0 0 0\n",
            "0 1 0 2 0\n",
            "0 1 0 2 0\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 1\n",
            "Value of next state according to MCTS : 0.3170\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 2 0 0 0\n",
            "0 1 0 0 0\n",
            "0 1 0 2 0\n",
            "0 1 0 2 0\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 3\n",
            "Value of next state according to MCTS : 1.1142\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 2 0 0 0\n",
            "0 1 0 1 0\n",
            "0 1 0 2 0\n",
            "0 1 0 2 0\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 4\n",
            "Value of next state according to MCTS : 0.2593\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 2 0 0 0\n",
            "0 1 0 1 0\n",
            "0 1 0 2 0\n",
            "0 1 0 2 2\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 1\n",
            "Value of next state according to MCTS : 0.8287\n",
            "0 0 0 0 0\n",
            "0 1 0 0 0\n",
            "0 2 0 0 0\n",
            "0 1 0 1 0\n",
            "0 1 0 2 0\n",
            "0 1 0 2 2\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 2\n",
            "Value of next state according to MCTS : 0.5679\n",
            "0 0 0 0 0\n",
            "0 1 0 0 0\n",
            "0 2 0 0 0\n",
            "0 1 0 1 0\n",
            "0 1 0 2 0\n",
            "0 1 2 2 2\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 0\n",
            "Value of next state according to MCTS : 1.0888\n",
            "0 0 0 0 0\n",
            "0 1 0 0 0\n",
            "0 2 0 0 0\n",
            "0 1 0 1 0\n",
            "0 1 0 2 0\n",
            "1 1 2 2 2\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 4\n",
            "Value of next state according to MCTS : 0.5549\n",
            "0 0 0 0 0\n",
            "0 1 0 0 0\n",
            "0 2 0 0 0\n",
            "0 1 0 1 0\n",
            "0 1 0 2 2\n",
            "1 1 2 2 2\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 1\n",
            "Value of next state according to MCTS : 0.9894\n",
            "0 1 0 0 0\n",
            "0 1 0 0 0\n",
            "0 2 0 0 0\n",
            "0 1 0 1 0\n",
            "0 1 0 2 2\n",
            "1 1 2 2 2\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 0\n",
            "Value of next state according to MCTS : 0.1894\n",
            "0 1 0 0 0\n",
            "0 1 0 0 0\n",
            "0 2 0 0 0\n",
            "0 1 0 1 0\n",
            "2 1 0 2 2\n",
            "1 1 2 2 2\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 4\n",
            "Value of next state according to MCTS : 0.8641\n",
            "0 1 0 0 0\n",
            "0 1 0 0 0\n",
            "0 2 0 0 0\n",
            "0 1 0 1 1\n",
            "2 1 0 2 2\n",
            "1 1 2 2 2\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 4\n",
            "Value of next state according to MCTS : 0.1637\n",
            "0 1 0 0 0\n",
            "0 1 0 0 0\n",
            "0 2 0 0 2\n",
            "0 1 0 1 1\n",
            "2 1 0 2 2\n",
            "1 1 2 2 2\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 3\n",
            "Value of next state according to MCTS : 0.9894\n",
            "0 1 0 0 0\n",
            "0 1 0 0 0\n",
            "0 2 0 1 2\n",
            "0 1 0 1 1\n",
            "2 1 0 2 2\n",
            "1 1 2 2 2\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 3\n",
            "Value of next state according to MCTS : 0.1304\n",
            "0 1 0 0 0\n",
            "0 1 0 2 0\n",
            "0 2 0 1 2\n",
            "0 1 0 1 1\n",
            "2 1 0 2 2\n",
            "1 1 2 2 2\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 4\n",
            "Value of next state according to MCTS : 0.9625\n",
            "0 1 0 0 0\n",
            "0 1 0 2 1\n",
            "0 2 0 1 2\n",
            "0 1 0 1 1\n",
            "2 1 0 2 2\n",
            "1 1 2 2 2\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 0\n",
            "Value of next state according to MCTS : -0.1254\n",
            "0 1 0 0 0\n",
            "0 1 0 2 1\n",
            "0 2 0 1 2\n",
            "2 1 0 1 1\n",
            "2 1 0 2 2\n",
            "1 1 2 2 2\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 0\n",
            "Value of next state according to MCTS : 0.9838\n",
            "0 1 0 0 0\n",
            "0 1 0 2 1\n",
            "1 2 0 1 2\n",
            "2 1 0 1 1\n",
            "2 1 0 2 2\n",
            "1 1 2 2 2\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 0\n",
            "Value of next state according to MCTS : -0.2522\n",
            "0 1 0 0 0\n",
            "2 1 0 2 1\n",
            "1 2 0 1 2\n",
            "2 1 0 1 1\n",
            "2 1 0 2 2\n",
            "1 1 2 2 2\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 3\n",
            "Value of next state according to MCTS : 1.2275\n",
            "0 1 0 1 0\n",
            "2 1 0 2 1\n",
            "1 2 0 1 2\n",
            "2 1 0 1 1\n",
            "2 1 0 2 2\n",
            "1 1 2 2 2\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 2\n",
            "Value of next state according to MCTS : -0.5536\n",
            "0 1 0 1 0\n",
            "2 1 0 2 1\n",
            "1 2 0 1 2\n",
            "2 1 0 1 1\n",
            "2 1 2 2 2\n",
            "1 1 2 2 2\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 2\n",
            "Value of next state according to MCTS : 1.4103\n",
            "0 1 0 1 0\n",
            "2 1 0 2 1\n",
            "1 2 0 1 2\n",
            "2 1 1 1 1\n",
            "2 1 2 2 2\n",
            "1 1 2 2 2\n",
            "\n",
            "Player 2 has Outcome.Loss. Total moves = 25.\n",
            "**** New Game *****\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 1\n",
            "Value of next state according to MCTS : 0.9016\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 1 0 0 0\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 2\n",
            "Value of next state according to MCTS : 0.3266\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 1 2 0 0\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 1\n",
            "Value of next state according to MCTS : 1.3014\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 1 0 0 0\n",
            "0 1 2 0 0\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 1\n",
            "Value of next state according to MCTS : 0.2544\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 2 0 0 0\n",
            "0 1 0 0 0\n",
            "0 1 2 0 0\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 2\n",
            "Value of next state according to MCTS : 1.2971\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 2 0 0 0\n",
            "0 1 1 0 0\n",
            "0 1 2 0 0\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 2\n",
            "Value of next state according to MCTS : 0.1840\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 2 2 0 0\n",
            "0 1 1 0 0\n",
            "0 1 2 0 0\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 4\n",
            "Value of next state according to MCTS : 0.9924\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 2 2 0 0\n",
            "0 1 1 0 0\n",
            "0 1 2 0 1\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 4\n",
            "Value of next state according to MCTS : 0.4362\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 2 2 0 0\n",
            "0 1 1 0 2\n",
            "0 1 2 0 1\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 1\n",
            "Value of next state according to MCTS : 0.7576\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 1 0 0 0\n",
            "0 2 2 0 0\n",
            "0 1 1 0 2\n",
            "0 1 2 0 1\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 2\n",
            "Value of next state according to MCTS : 0.6660\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 1 2 0 0\n",
            "0 2 2 0 0\n",
            "0 1 1 0 2\n",
            "0 1 2 0 1\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 4\n",
            "Value of next state according to MCTS : 0.8662\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 1 2 0 0\n",
            "0 2 2 0 1\n",
            "0 1 1 0 2\n",
            "0 1 2 0 1\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 2\n",
            "Value of next state according to MCTS : 0.6634\n",
            "0 0 0 0 0\n",
            "0 0 2 0 0\n",
            "0 1 2 0 0\n",
            "0 2 2 0 1\n",
            "0 1 1 0 2\n",
            "0 1 2 0 1\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 2\n",
            "Value of next state according to MCTS : 0.8046\n",
            "0 0 1 0 0\n",
            "0 0 2 0 0\n",
            "0 1 2 0 0\n",
            "0 2 2 0 1\n",
            "0 1 1 0 2\n",
            "0 1 2 0 1\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 4\n",
            "Value of next state according to MCTS : 0.4529\n",
            "0 0 1 0 0\n",
            "0 0 2 0 0\n",
            "0 1 2 0 2\n",
            "0 2 2 0 1\n",
            "0 1 1 0 2\n",
            "0 1 2 0 1\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 1\n",
            "Value of next state according to MCTS : 0.5199\n",
            "0 0 1 0 0\n",
            "0 1 2 0 0\n",
            "0 1 2 0 2\n",
            "0 2 2 0 1\n",
            "0 1 1 0 2\n",
            "0 1 2 0 1\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 1\n",
            "Value of next state according to MCTS : 0.5033\n",
            "0 2 1 0 0\n",
            "0 1 2 0 0\n",
            "0 1 2 0 2\n",
            "0 2 2 0 1\n",
            "0 1 1 0 2\n",
            "0 1 2 0 1\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 4\n",
            "Value of next state according to MCTS : 0.6716\n",
            "0 2 1 0 0\n",
            "0 1 2 0 1\n",
            "0 1 2 0 2\n",
            "0 2 2 0 1\n",
            "0 1 1 0 2\n",
            "0 1 2 0 1\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 4\n",
            "Value of next state according to MCTS : 0.4904\n",
            "0 2 1 0 2\n",
            "0 1 2 0 1\n",
            "0 1 2 0 2\n",
            "0 2 2 0 1\n",
            "0 1 1 0 2\n",
            "0 1 2 0 1\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 3\n",
            "Value of next state according to MCTS : 0.2695\n",
            "0 2 1 0 2\n",
            "0 1 2 0 1\n",
            "0 1 2 0 2\n",
            "0 2 2 0 1\n",
            "0 1 1 0 2\n",
            "0 1 2 1 1\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 3\n",
            "Value of next state according to MCTS : 0.4677\n",
            "0 2 1 0 2\n",
            "0 1 2 0 1\n",
            "0 1 2 0 2\n",
            "0 2 2 0 1\n",
            "0 1 1 2 2\n",
            "0 1 2 1 1\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 3\n",
            "Value of next state according to MCTS : 0.0791\n",
            "0 2 1 0 2\n",
            "0 1 2 0 1\n",
            "0 1 2 0 2\n",
            "0 2 2 1 1\n",
            "0 1 1 2 2\n",
            "0 1 2 1 1\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 0\n",
            "Value of next state according to MCTS : 0.3615\n",
            "0 2 1 0 2\n",
            "0 1 2 0 1\n",
            "0 1 2 0 2\n",
            "0 2 2 1 1\n",
            "0 1 1 2 2\n",
            "2 1 2 1 1\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 0\n",
            "Value of next state according to MCTS : 0.1547\n",
            "0 2 1 0 2\n",
            "0 1 2 0 1\n",
            "0 1 2 0 2\n",
            "0 2 2 1 1\n",
            "1 1 1 2 2\n",
            "2 1 2 1 1\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 0\n",
            "Value of next state according to MCTS : 0.3070\n",
            "0 2 1 0 2\n",
            "0 1 2 0 1\n",
            "0 1 2 0 2\n",
            "2 2 2 1 1\n",
            "1 1 1 2 2\n",
            "2 1 2 1 1\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 0\n",
            "Value of next state according to MCTS : 0.2223\n",
            "0 2 1 0 2\n",
            "0 1 2 0 1\n",
            "1 1 2 0 2\n",
            "2 2 2 1 1\n",
            "1 1 1 2 2\n",
            "2 1 2 1 1\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 3\n",
            "Value of next state according to MCTS : 0.2719\n",
            "0 2 1 0 2\n",
            "0 1 2 0 1\n",
            "1 1 2 2 2\n",
            "2 2 2 1 1\n",
            "1 1 1 2 2\n",
            "2 1 2 1 1\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 3\n",
            "Value of next state according to MCTS : 0.3631\n",
            "0 2 1 0 2\n",
            "0 1 2 1 1\n",
            "1 1 2 2 2\n",
            "2 2 2 1 1\n",
            "1 1 1 2 2\n",
            "2 1 2 1 1\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 0\n",
            "Value of next state according to MCTS : 0.2398\n",
            "0 2 1 0 2\n",
            "2 1 2 1 1\n",
            "1 1 2 2 2\n",
            "2 2 2 1 1\n",
            "1 1 1 2 2\n",
            "2 1 2 1 1\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 0\n",
            "Value of next state according to MCTS : 0.4825\n",
            "1 2 1 0 2\n",
            "2 1 2 1 1\n",
            "1 1 2 2 2\n",
            "2 2 2 1 1\n",
            "1 1 1 2 2\n",
            "2 1 2 1 1\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 3\n",
            "Value of next state according to MCTS : 0.1925\n",
            "1 2 1 2 2\n",
            "2 1 2 1 1\n",
            "1 1 2 2 2\n",
            "2 2 2 1 1\n",
            "1 1 1 2 2\n",
            "2 1 2 1 1\n",
            "\n",
            "Player 1 has Outcome.Draw. Total moves = 30.\n",
            "**** New Game *****\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 1\n",
            "Value of next state according to MCTS : 1.4116\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 1 0 0 0\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 3\n",
            "Value of next state according to MCTS : 0.2995\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 1 0 2 0\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 1\n",
            "Value of next state according to MCTS : 0.9733\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 1 0 0 0\n",
            "0 1 0 2 0\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 3\n",
            "Value of next state according to MCTS : 0.1884\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 1 0 2 0\n",
            "0 1 0 2 0\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 1\n",
            "Value of next state according to MCTS : 0.8997\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 1 0 0 0\n",
            "0 1 0 2 0\n",
            "0 1 0 2 0\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 1\n",
            "Value of next state according to MCTS : 0.0964\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 2 0 0 0\n",
            "0 1 0 0 0\n",
            "0 1 0 2 0\n",
            "0 1 0 2 0\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 3\n",
            "Value of next state according to MCTS : 0.7857\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 2 0 0 0\n",
            "0 1 0 1 0\n",
            "0 1 0 2 0\n",
            "0 1 0 2 0\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 2\n",
            "Value of next state according to MCTS : 0.1834\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 2 0 0 0\n",
            "0 1 0 1 0\n",
            "0 1 0 2 0\n",
            "0 1 2 2 0\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 1\n",
            "Value of next state according to MCTS : 1.0426\n",
            "0 0 0 0 0\n",
            "0 1 0 0 0\n",
            "0 2 0 0 0\n",
            "0 1 0 1 0\n",
            "0 1 0 2 0\n",
            "0 1 2 2 0\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 4\n",
            "Value of next state according to MCTS : 0.3272\n",
            "0 0 0 0 0\n",
            "0 1 0 0 0\n",
            "0 2 0 0 0\n",
            "0 1 0 1 0\n",
            "0 1 0 2 0\n",
            "0 1 2 2 2\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 4\n",
            "Value of next state according to MCTS : 1.1655\n",
            "0 0 0 0 0\n",
            "0 1 0 0 0\n",
            "0 2 0 0 0\n",
            "0 1 0 1 0\n",
            "0 1 0 2 1\n",
            "0 1 2 2 2\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 0\n",
            "Value of next state according to MCTS : 0.2791\n",
            "0 0 0 0 0\n",
            "0 1 0 0 0\n",
            "0 2 0 0 0\n",
            "0 1 0 1 0\n",
            "0 1 0 2 1\n",
            "2 1 2 2 2\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 0\n",
            "Value of next state according to MCTS : 0.9883\n",
            "0 0 0 0 0\n",
            "0 1 0 0 0\n",
            "0 2 0 0 0\n",
            "0 1 0 1 0\n",
            "1 1 0 2 1\n",
            "2 1 2 2 2\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 0\n",
            "Value of next state according to MCTS : 0.2820\n",
            "0 0 0 0 0\n",
            "0 1 0 0 0\n",
            "0 2 0 0 0\n",
            "2 1 0 1 0\n",
            "1 1 0 2 1\n",
            "2 1 2 2 2\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 3\n",
            "Value of next state according to MCTS : 1.1060\n",
            "0 0 0 0 0\n",
            "0 1 0 0 0\n",
            "0 2 0 1 0\n",
            "2 1 0 1 0\n",
            "1 1 0 2 1\n",
            "2 1 2 2 2\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 3\n",
            "Value of next state according to MCTS : 0.2881\n",
            "0 0 0 0 0\n",
            "0 1 0 2 0\n",
            "0 2 0 1 0\n",
            "2 1 0 1 0\n",
            "1 1 0 2 1\n",
            "2 1 2 2 2\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 0\n",
            "Value of next state according to MCTS : 1.0542\n",
            "0 0 0 0 0\n",
            "0 1 0 2 0\n",
            "1 2 0 1 0\n",
            "2 1 0 1 0\n",
            "1 1 0 2 1\n",
            "2 1 2 2 2\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 3\n",
            "Value of next state according to MCTS : 0.2017\n",
            "0 0 0 2 0\n",
            "0 1 0 2 0\n",
            "1 2 0 1 0\n",
            "2 1 0 1 0\n",
            "1 1 0 2 1\n",
            "2 1 2 2 2\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 4\n",
            "Value of next state according to MCTS : 1.3056\n",
            "0 0 0 2 0\n",
            "0 1 0 2 0\n",
            "1 2 0 1 0\n",
            "2 1 0 1 1\n",
            "1 1 0 2 1\n",
            "2 1 2 2 2\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 4\n",
            "Value of next state according to MCTS : -0.1779\n",
            "0 0 0 2 0\n",
            "0 1 0 2 0\n",
            "1 2 0 1 2\n",
            "2 1 0 1 1\n",
            "1 1 0 2 1\n",
            "2 1 2 2 2\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 4\n",
            "Value of next state according to MCTS : 1.0844\n",
            "0 0 0 2 0\n",
            "0 1 0 2 1\n",
            "1 2 0 1 2\n",
            "2 1 0 1 1\n",
            "1 1 0 2 1\n",
            "2 1 2 2 2\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 0\n",
            "Value of next state according to MCTS : -0.1807\n",
            "0 0 0 2 0\n",
            "2 1 0 2 1\n",
            "1 2 0 1 2\n",
            "2 1 0 1 1\n",
            "1 1 0 2 1\n",
            "2 1 2 2 2\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 4\n",
            "Value of next state according to MCTS : 1.2187\n",
            "0 0 0 2 1\n",
            "2 1 0 2 1\n",
            "1 2 0 1 2\n",
            "2 1 0 1 1\n",
            "1 1 0 2 1\n",
            "2 1 2 2 2\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 2\n",
            "Value of next state according to MCTS : -0.5208\n",
            "0 0 0 2 1\n",
            "2 1 0 2 1\n",
            "1 2 0 1 2\n",
            "2 1 0 1 1\n",
            "1 1 2 2 1\n",
            "2 1 2 2 2\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 2\n",
            "Value of next state according to MCTS : 1.4223\n",
            "0 0 0 2 1\n",
            "2 1 0 2 1\n",
            "1 2 0 1 2\n",
            "2 1 1 1 1\n",
            "1 1 2 2 1\n",
            "2 1 2 2 2\n",
            "\n",
            "Player 2 has Outcome.Loss. Total moves = 25.\n",
            "**** New Game *****\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 2\n",
            "Value of next state according to MCTS : 1.2842\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 1 0 0\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 3\n",
            "Value of next state according to MCTS : 0.3178\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 1 2 0\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 3\n",
            "Value of next state according to MCTS : 1.0682\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 1 0\n",
            "0 0 1 2 0\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 2\n",
            "Value of next state according to MCTS : 0.3036\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 2 1 0\n",
            "0 0 1 2 0\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 2\n",
            "Value of next state according to MCTS : 0.7642\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 1 0 0\n",
            "0 0 2 1 0\n",
            "0 0 1 2 0\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 3\n",
            "Value of next state according to MCTS : 0.2801\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 1 2 0\n",
            "0 0 2 1 0\n",
            "0 0 1 2 0\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 2\n",
            "Value of next state according to MCTS : 0.9954\n",
            "0 0 0 0 0\n",
            "0 0 0 0 0\n",
            "0 0 1 0 0\n",
            "0 0 1 2 0\n",
            "0 0 2 1 0\n",
            "0 0 1 2 0\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 2\n",
            "Value of next state according to MCTS : 0.2696\n",
            "0 0 0 0 0\n",
            "0 0 2 0 0\n",
            "0 0 1 0 0\n",
            "0 0 1 2 0\n",
            "0 0 2 1 0\n",
            "0 0 1 2 0\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 3\n",
            "Value of next state according to MCTS : 1.0590\n",
            "0 0 0 0 0\n",
            "0 0 2 0 0\n",
            "0 0 1 1 0\n",
            "0 0 1 2 0\n",
            "0 0 2 1 0\n",
            "0 0 1 2 0\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 0\n",
            "Value of next state according to MCTS : 0.2795\n",
            "0 0 0 0 0\n",
            "0 0 2 0 0\n",
            "0 0 1 1 0\n",
            "0 0 1 2 0\n",
            "0 0 2 1 0\n",
            "2 0 1 2 0\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 1\n",
            "Value of next state according to MCTS : 0.8741\n",
            "0 0 0 0 0\n",
            "0 0 2 0 0\n",
            "0 0 1 1 0\n",
            "0 0 1 2 0\n",
            "0 0 2 1 0\n",
            "2 1 1 2 0\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 0\n",
            "Value of next state according to MCTS : 0.3692\n",
            "0 0 0 0 0\n",
            "0 0 2 0 0\n",
            "0 0 1 1 0\n",
            "0 0 1 2 0\n",
            "2 0 2 1 0\n",
            "2 1 1 2 0\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 2\n",
            "Value of next state according to MCTS : 0.9309\n",
            "0 0 1 0 0\n",
            "0 0 2 0 0\n",
            "0 0 1 1 0\n",
            "0 0 1 2 0\n",
            "2 0 2 1 0\n",
            "2 1 1 2 0\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 0\n",
            "Value of next state according to MCTS : 0.4499\n",
            "0 0 1 0 0\n",
            "0 0 2 0 0\n",
            "0 0 1 1 0\n",
            "2 0 1 2 0\n",
            "2 0 2 1 0\n",
            "2 1 1 2 0\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 0\n",
            "Value of next state according to MCTS : 0.8474\n",
            "0 0 1 0 0\n",
            "0 0 2 0 0\n",
            "1 0 1 1 0\n",
            "2 0 1 2 0\n",
            "2 0 2 1 0\n",
            "2 1 1 2 0\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 3\n",
            "Value of next state according to MCTS : 0.0475\n",
            "0 0 1 0 0\n",
            "0 0 2 2 0\n",
            "1 0 1 1 0\n",
            "2 0 1 2 0\n",
            "2 0 2 1 0\n",
            "2 1 1 2 0\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 3\n",
            "Value of next state according to MCTS : 1.0780\n",
            "0 0 1 1 0\n",
            "0 0 2 2 0\n",
            "1 0 1 1 0\n",
            "2 0 1 2 0\n",
            "2 0 2 1 0\n",
            "2 1 1 2 0\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 1\n",
            "Value of next state according to MCTS : -0.1272\n",
            "0 0 1 1 0\n",
            "0 0 2 2 0\n",
            "1 0 1 1 0\n",
            "2 0 1 2 0\n",
            "2 2 2 1 0\n",
            "2 1 1 2 0\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 4\n",
            "Value of next state according to MCTS : 1.0418\n",
            "0 0 1 1 0\n",
            "0 0 2 2 0\n",
            "1 0 1 1 0\n",
            "2 0 1 2 0\n",
            "2 2 2 1 0\n",
            "2 1 1 2 1\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 0\n",
            "Value of next state according to MCTS : 0.0339\n",
            "0 0 1 1 0\n",
            "2 0 2 2 0\n",
            "1 0 1 1 0\n",
            "2 0 1 2 0\n",
            "2 2 2 1 0\n",
            "2 1 1 2 1\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 0\n",
            "Value of next state according to MCTS : 1.1227\n",
            "1 0 1 1 0\n",
            "2 0 2 2 0\n",
            "1 0 1 1 0\n",
            "2 0 1 2 0\n",
            "2 2 2 1 0\n",
            "2 1 1 2 1\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 4\n",
            "Value of next state according to MCTS : 0.1133\n",
            "1 0 1 1 0\n",
            "2 0 2 2 0\n",
            "1 0 1 1 0\n",
            "2 0 1 2 0\n",
            "2 2 2 1 2\n",
            "2 1 1 2 1\n",
            "\n",
            "Player 1 (MCTS)\n",
            "Action Selected : 4\n",
            "Value of next state according to MCTS : 0.8084\n",
            "1 0 1 1 0\n",
            "2 0 2 2 0\n",
            "1 0 1 1 0\n",
            "2 0 1 2 1\n",
            "2 2 2 1 2\n",
            "2 1 1 2 1\n",
            "\n",
            "Player 2 (MCTS)\n",
            "Action Selected : 4\n",
            "Value of next state according to MCTS : 0.1337\n",
            "1 0 1 1 0\n",
            "2 0 2 2 0\n",
            "1 0 1 1 2\n",
            "2 0 1 2 1\n",
            "2 2 2 1 2\n",
            "2 1 1 2 1\n",
            "\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_335/3843002363.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Agent 0 is 40\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMonteCarloTreeSearchAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muctConst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMonteCarloTreeSearchAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muctConst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_335/4154169076.py\u001b[0m in \u001b[0;36mplay\u001b[0;34m(board, firstAgent, secondAgent)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mnumMoves\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mmove\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrAgentIndex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetMove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0magents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mcurrAgentIndex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregisterMove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_335/2402384162.py\u001b[0m in \u001b[0;36mgetMove\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m4\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirstMove\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m               \u001b[0mcurrNode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpansion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrNode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurrNode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0moutcome\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimulation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrBoard\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurrNode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_335/2402384162.py\u001b[0m in \u001b[0;36mexpansion\u001b[0;34m(self, currNode)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexpansion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrNode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mcurrNode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOutcome\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mOutcome\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNonLeaf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m             \u001b[0mcurrNode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomputeChildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcurrNode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselectChild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muctConst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_335/3783448753.py\u001b[0m in \u001b[0;36mgetOutcome\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetOutcome\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwinOrLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0muctValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muctConstant\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_335/3632151451.py\u001b[0m in \u001b[0;36mwinOrLoss\u001b[0;34m(grid)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mtempOutcome\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckDiagonal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtempOutcome\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mOutcome\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDraw\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtempOutcome\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_335/3632151451.py\u001b[0m in \u001b[0;36mcheckDiagonal\u001b[0;34m(grid, startRow, startCol, rowUpdate, colUpdate)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstartCol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;34m>=\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mconsecutiveNegOnes\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mconsecutiveOnes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "uctConsts = [1,1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8,1.9,2]\n",
        "\n",
        "scores = []\n",
        "\n",
        "for uctConst in uctConsts:\n",
        "    score = [0,0,0]\n",
        "    print(\"Agent 0 is 40\")\n",
        "    for i in range(50):\n",
        "        score[play(np.zeros((6,5)).astype(int), MonteCarloTreeSearchAgent(40, uctConst), MonteCarloTreeSearchAgent(200, uctConst))]+=1\n",
        "    print(score)\n",
        "\n",
        "    print(\"Agent 0 is 200\")\n",
        "    for i in range(50):\n",
        "        won = play(np.zeros((6,5)).astype(int), MonteCarloTreeSearchAgent(200, uctConst), MonteCarloTreeSearchAgent(40, uctConst))\n",
        "        if won!=2:\n",
        "            score[1-won]+=1\n",
        "        else:\n",
        "            score[2]+=1\n",
        "    print(score)\n",
        "\n",
        "    scores.append(score)\n",
        "\n",
        "scores_final = [scores[index][1]/scores[index][0] for index in range(len(scores))]\n",
        "plt.plot(uctConsts, scores_final)\n",
        "plt.xlabel(\"UCT Const\")\n",
        "plt.ylabel(\"Wins/Losses\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_XmYIxPW2WKI",
        "outputId": "11186604-2ed8-4c9f-e7dc-e13ede455c07"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_3554/1499631668.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgammas\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAfterstatesQLearningAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMonteCarloTreeSearchAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0max1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipykernel_3554/1499631668.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(QAgent, MCAgent, gamma, alpha, ax1, ax2)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mQAgent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mplayNewGame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMCAgent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQAgent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_3554/3964726474.py\u001b[0m in \u001b[0;36mplayNewGame\u001b[0;34m(board, firstAgent, secondAgent)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mwinOrLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mOutcome\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNonLeaf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mmove\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrAgentIndex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetMove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0magents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mcurrAgentIndex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregisterMove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_3554/188478201.py\u001b[0m in \u001b[0;36mgetMove\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     27\u001b[0m               \u001b[0mcurrNode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpansion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrNode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurrNode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0moutcome\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimulation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrBoard\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurrNode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrNode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurrNode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutcome\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutcome\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_3554/188478201.py\u001b[0m in \u001b[0;36msimulation\u001b[0;34m(self, currBoard)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mnextBoard\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreateChild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrBoard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0moutcome\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimulation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnextBoard\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moutcome\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mOutcome\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mOutcome\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_3554/188478201.py\u001b[0m in \u001b[0;36msimulation\u001b[0;34m(self, currBoard)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mnextBoard\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreateChild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrBoard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0moutcome\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimulation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnextBoard\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moutcome\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mOutcome\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mOutcome\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_3554/188478201.py\u001b[0m in \u001b[0;36msimulation\u001b[0;34m(self, currBoard)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mnextBoard\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreateChild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrBoard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0moutcome\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimulation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnextBoard\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moutcome\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mOutcome\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mOutcome\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_3554/188478201.py\u001b[0m in \u001b[0;36msimulation\u001b[0;34m(self, currBoard)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mnextBoard\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreateChild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrBoard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0moutcome\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimulation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnextBoard\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moutcome\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mOutcome\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mOutcome\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_3554/188478201.py\u001b[0m in \u001b[0;36msimulation\u001b[0;34m(self, currBoard)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mnextBoard\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreateChild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrBoard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0moutcome\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimulation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnextBoard\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moutcome\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mOutcome\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mOutcome\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_3554/188478201.py\u001b[0m in \u001b[0;36msimulation\u001b[0;34m(self, currBoard)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msimulation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrBoard\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mwinOrLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrBoard\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mOutcome\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNonLeaf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m             \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetAvailableMoves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrBoard\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mnextBoard\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreateChild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrBoard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_3554/3632151451.py\u001b[0m in \u001b[0;36mwinOrLoss\u001b[0;34m(grid)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mtempOutcome\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckDiagonal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtempOutcome\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mOutcome\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDraw\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtempOutcome\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_3554/3632151451.py\u001b[0m in \u001b[0;36mcheckDiagonal\u001b[0;34m(grid, startRow, startCol, rowUpdate, colUpdate)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstartRow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstartCol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mwhile\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;34m>=\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mconsecutiveNegOnes\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1440x576 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "def train(QAgent, MCAgent, gamma, alpha,ax1,ax2):\n",
        "\n",
        "    scores = []\n",
        "    for epsilon in np.linspace(1, 0, 11):\n",
        "        score = [0,0,0]\n",
        "        QAgent.epsilon = epsilon\n",
        "        for i in range(100):\n",
        "            score[play(np.zeros((5,4)).astype(int), MCAgent, QAgent)]+=1\n",
        "          \n",
        "        scores.append(score)\n",
        "    \n",
        "    wins_final = [scores[index][1] for index in range(len(scores))]\n",
        "    loss_final = [scores[index][0] for index in range(len(scores))]\n",
        "    ax1.plot(np.linspace(1000, 10000, 11), wins_final, label = \"Alpha=\"+\"{:.2f}\".format(alpha)+\"Gamma\"+\"{:.2f}\".format(gamma))\n",
        "    ax1.set_xlabel(\"Iterations\")\n",
        "    ax1.set_ylabel(\"Wins\")\n",
        "    ax1.set_title(\"Wins\")\n",
        "    ax1.legend()\n",
        "\n",
        "    ax2.plot(np.linspace(1000, 10000, 11), loss_final, label = \"Alpha=\"+\"{:.2f}\".format(alpha)+\"Gamma\"+\"{:.2f}\".format(gamma))\n",
        "    ax2.set_xlabel(\"Iterations\")\n",
        "    ax2.set_ylabel(\"Losses\")\n",
        "    ax2.set_title(\"Losses\")\n",
        "    ax2.legend()\n",
        "\n",
        "gammas = np.linspace(0, 1, 4)\n",
        "alphas = np.linspace(0, 1, 4)\n",
        "\n",
        "for alpha in alphas:\n",
        "    fig = plt.figure(figsize=(20,8))\n",
        "    ax1 = fig.add_subplot(1, 2, 1)\n",
        "    ax2 = fig.add_subplot(1, 2, 2)\n",
        "    for gamma in gammas:\n",
        "        agent = AfterstatesQLearningAgent(epsilon=1, gamma=gamma, alpha=alpha)\n",
        "        train(agent, MonteCarloTreeSearchAgent(25), gamma, alpha,ax1, ax2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 688
        },
        "id": "bNKNEloH6D8M",
        "outputId": "f3e54671-7fdb-4816-94f9-ea6e4e0a5070"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-a8f4ad2b0fd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0max2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmcnum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmcnums\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAfterstatesQLearningAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMonteCarloTreeSearchAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmcnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0max1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'gamma' is not defined"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1440x576 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "def train(QAgent, MCAgent, gamma, alpha,ax1,ax2):\n",
        "\n",
        "    scores = []\n",
        "    for epsilon in np.linspace(1, 0, 11):\n",
        "        score = [0,0,0]\n",
        "        QAgent.epsilon = epsilon\n",
        "        for i in range(100):\n",
        "            score[play(np.zeros((5,4)).astype(int), MCAgent, QAgent)]+=1\n",
        "          \n",
        "        scores.append(score)\n",
        "    \n",
        "    wins_final = [scores[index][1] for index in range(len(scores))]\n",
        "    loss_final = [scores[index][0] for index in range(len(scores))]\n",
        "    ax1.plot(np.linspace(1000, 10000, 11), wins_final, label = \"Alpha=\"+\"{:.2f}\".format(alpha)+\"Gamma\"+\"{:.2f}\".format(gamma))\n",
        "    ax1.set_xlabel(\"Iterations\")\n",
        "    ax1.set_ylabel(\"Wins\")\n",
        "    ax1.set_title(\"Wins\")\n",
        "    ax1.legend()\n",
        "\n",
        "    ax2.plot(np.linspace(1000, 10000, 11), loss_final, label = \"Alpha=\"+\"{:.2f}\".format(alpha)+\"Gamma\"+\"{:.2f}\".format(gamma))\n",
        "    ax2.set_xlabel(\"Iterations\")\n",
        "    ax2.set_ylabel(\"Losses\")\n",
        "    ax2.set_title(\"Losses\")\n",
        "    ax2.legend()\n",
        "\n",
        "mcnums = [1, 5, 10, 15, 20 ,25]\n",
        "\n",
        "fig = plt.figure(figsize=(20,8))\n",
        "ax1 = fig.add_subplot(1, 2, 1)\n",
        "ax2 = fig.add_subplot(1, 2, 2)\n",
        "for mcnum in mcnums:\n",
        "        agent = AfterstatesQLearningAgent(epsilon=1, gamma=gamma, alpha=alpha)\n",
        "        train(agent, MonteCarloTreeSearchAgent(mcnum), gamma, alpha,ax1, ax2)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "sarthak.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
